# üéØ Quick Start Guide - RAGAS Evaluation

Get started with RAGAS evaluation in 5 minutes!

## Step 1: Install Dependencies

```bash
pip install -r requirements.txt
```

## Step 2: Set OpenAI API Key

**Option A - Environment Variable (Recommended)**
```powershell
# Windows PowerShell
$env:OPENAI_API_KEY="sk-your-api-key-here"
```

**Option B - .env File**
```bash
# Copy example file
cp .env.example .env

# Edit .env and add your key
# OPENAI_API_KEY=sk-your-api-key-here
```

## Step 3: Run Your First Evaluation

### Quick Test (3 questions, ~30 seconds)
```bash
python evaluate_rag.py --quick
```

### Full Evaluation (all test questions)
```bash
python evaluate_rag.py
```

### Test a Single Query
```bash
python evaluate_rag.py --query "Which fintech startups got funding in Karnataka?"
```

## üìä What You'll See

```
üöÄ RAGAS EVALUATION FOR RAG SYSTEM
================================================

1Ô∏è‚É£  Initializing RAG system...
‚úÖ RAG system loaded successfully!

2Ô∏è‚É£  Initializing RAGAS evaluator...
‚úÖ Initialized LLM: gpt-3.5-turbo

3Ô∏è‚É£  Preparing test dataset...
   Using quick test set: 3 questions

4Ô∏è‚É£  Running queries through RAG system...
   Processing 1/3: Which fintech startups...
   Processing 2/3: What was the total...
   Processing 3/3: ‡§ï‡§∞‡•ç‡§®‡§æ‡§ü‡§ï ‡§Æ‡•á‡§Ç ‡§´‡§Ç‡§°‡§ø‡§Ç‡§ó...

5Ô∏è‚É£  Preparing evaluation dataset...

6Ô∏è‚É£  Running RAGAS evaluation...
üìä Evaluating 3 samples
üìè Metrics: [faithfulness, answer_relevancy, ...]

‚úÖ Evaluation completed successfully!

============================================================
üìä RAGAS EVALUATION REPORT
============================================================

üîç RETRIEVAL METRICS:
  context_precision   : 0.8542 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
  context_recall      : 0.7891 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
  context_relevancy   : 0.8123 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

üìù GENERATION METRICS:
  faithfulness        : 0.9156 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
  answer_relevancy    : 0.8234 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
  answer_correctness  : 0.7645 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

------------------------------------------------------------
üéØ OVERALL AVERAGE: 0.8229
============================================================

üíæ Results exported to: evaluation_results_20260103_143022.csv
```

## üéì Understanding Your Scores

| Score Range | Quality | Action |
|------------|---------|--------|
| 0.8 - 1.0 | Excellent ‚úÖ | Keep it up! |
| 0.6 - 0.8 | Good üëç | Minor tweaks |
| 0.4 - 0.6 | Fair ‚ö†Ô∏è | Needs improvement |
| < 0.4 | Poor ‚ùå | Requires attention |

## üîß Common Issues

### "OpenAI API Key not found"
**Solution**: Set the environment variable
```powershell
$env:OPENAI_API_KEY="your-key"
```

### "Module 'ragas' not found"
**Solution**: Install dependencies
```bash
pip install -r requirements.txt
```

### "Vector index not found"
**Solution**: Make sure your RAG system is set up with data
```python
from src.rag import build_vector_index
from src.data_loader import load_and_clean_data

df = load_and_clean_data()
index, chunks = build_vector_index(df)
```

## üìö Next Steps

1. ‚úÖ Run quick evaluation
2. üìñ Read detailed guide: [RAGAS_GUIDE.md](RAGAS_GUIDE.md)
3. üß™ Add your own test cases: Edit `src/test_dataset.py`
4. üéØ Improve your scores: Optimize retrieval and generation
5. üìä Track progress: Compare results over time

## üí° Pro Tips

1. **Start Small**: Use `--quick` flag first
2. **Monitor Costs**: Each evaluation costs ~$0.01-$0.20
3. **Baseline First**: Run initial evaluation before making changes
4. **Iterate**: Make small improvements and re-evaluate
5. **Track Results**: Keep CSV exports for comparison

## üéØ Evaluation Workflow

```
1. Baseline Evaluation
   ‚Üì
2. Identify Weak Metrics
   ‚Üì
3. Make Improvements
   ‚Üì
4. Re-evaluate
   ‚Üì
5. Compare Results
   ‚Üì
6. Repeat!
```

## üìû Need Help?

- üìñ Full Guide: [RAGAS_GUIDE.md](RAGAS_GUIDE.md)
- üîß Code: Check `src/ragas_evaluator.py`
- üß™ Tests: See `src/test_dataset.py`
- üìä Runner: Check `evaluate_rag.py`

---

**Ready to improve your RAG system? Let's go! üöÄ**
